{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#from keras import backend as K\n",
    "#from keras.models import Sequential, Model\n",
    "#from keras.layers import Input, LSTM, RepeatVector\n",
    "#from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "#from keras.optimizers import SGD, RMSprop, Adam\n",
    "#from keras import objectives\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout, Masking\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiH2][BiH2].C1(=CC=C(C=C1)(C(O)=O))(C(O)=O).COc1cc((C(O)=O))c(OC)cc1(C(O)=O) MOFid-v1.bew.cat0;comment\n"
     ]
    }
   ],
   "source": [
    "imported_data = pd.read_hdf(\"test_database.h5\")\n",
    "mofids = imported_data[\"MOFid\"].tolist()\n",
    "#print(imported_data)\n",
    "print(mofids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiH2][BiH2].C1(=CC=C(C=C1)(C(O)=O))(C(O)=O).COc1cc((C(O)=O))c(OC)cc1(C(O)=O).bew\n"
     ]
    }
   ],
   "source": [
    "def clean_inputs(mofid, cleaned_list):\n",
    "    for i in range(len(mofid)):\n",
    "        unclean = mofid[i]\n",
    "        spaces_removed = unclean.replace(' ','')\n",
    "        version_removed = spaces_removed.replace('MOFid-v1', '')\n",
    "        comment_removed = version_removed.replace(';comment', '')\n",
    "        clean = comment_removed.replace('.cat0', '')\n",
    "        cleaned_list.append(clean)\n",
    "\n",
    "clean_mofids = []\n",
    "clean_inputs(mofids, clean_mofids)\n",
    "max_mofid_length = max([len(mofid) for mofid in clean_mofids])\n",
    "print(clean_mofids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 unique tokens\n",
      "Shape of data tensor: (10651, 485)\n",
      "(53, 52)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(num_words=300, char_level=True, lower=False, oov_token='UNK')\n",
    "tk.fit_on_texts(clean_mofids)\n",
    "char_index = tk.word_index\n",
    "index2char = {v: k for k, v in char_index.items()}\n",
    "\n",
    "print(\"Found %s unique tokens\" % len(char_index))\n",
    "\n",
    "full_sequences = tk.texts_to_sequences(clean_mofids)\n",
    "full_data = pad_sequences(full_sequences, maxlen=max_mofid_length, padding='post')\n",
    "train_data = np.array(full_data, dtype=\"int\")\n",
    "\n",
    "train_split, val_split = train_test_split(train_data, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print('Shape of data tensor:', full_data.shape)\n",
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "\n",
    "print(embedding_weights.shape)\n",
    "print(embedding_weights)\n",
    "\n",
    "#NB_CHARS = (min(tk.num_words, len(char_index)) + 1 ) # +1 to account for zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "input_characters = set()\n",
    "input_characters.add('UNK')\n",
    "\n",
    "for i in range(len(clean_mofids)):\n",
    "    input_text = clean_mofids[i]\n",
    "    input_texts.append(input_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i+1) for i, char in enumerate(input_characters)])\n",
    "print(input_token_index)\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens+1), dtype=\"int\"\n",
    ")\n",
    "\n",
    "print(input_texts[0])\n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(len(encoder_input_data[0][0]))\n",
    "np.set_printoptions(threshold=1000)\n",
    "\n",
    "train_data, validation_data = train_test_split(encoder_input_data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LSTM network expects the input to be in the form [samples, time steps, features] where samples is the number of data points we have, time steps is the number of time-dependent steps that are there in a single data point, features refers to the number of variables we have for the corresponding true value in Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION_SPLIT = 0.2\n",
    "#MAX_SEQUENCE_LENGTH = max_mofid_length\n",
    "#MAX_NB_WORDS = len(train_data)\n",
    "#EMBEDDING_DIM = 300\n",
    "#NB_CHARS = len(char_index) + 1\n",
    "#emb_dim = EMBEDDING_DIM\n",
    "\n",
    "batch_size = 1000\n",
    "max_len = max_mofid_length\n",
    "embedding_size = vocab_size\n",
    "latent_dim = 26\n",
    "intermediate_dim = 53\n",
    "epsilon_std = 1.0\n",
    "kl_weight = 0.01\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "\n",
    "#timesteps = max_mofid_length\n",
    "#features = NB_CHARS\n",
    "\n",
    "x = Input(shape=(max_len,))\n",
    "x_embed = Embedding(vocab_size+1, embedding_size, mask_zero=True, input_length=max_len, weights=[embedding_weights])(x)\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "#h = Dropout(0.2)(h)\n",
    "#h = Dense(intermediate_dim, activation='linear')(h)\n",
    "#h = act(h)\n",
    "#h = Dropout(0.2)(h)\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(h)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(h)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 485) (1000, 485, 53)\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 485)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 485, 52)      2756        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 106)          44944       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 26)           2782        bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 26)           2782        bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 26)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 485, 26)      0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (None, 485, 52)      16432       repeat_vector_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 485, 53)      2809        lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_7 (Cus [(None, 485), (None, 0           input_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 72,505\n",
      "Trainable params: 72,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(vocab_size, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = Dense(vocab_size+1, activation='linear')#softmax is applied in the seq2seqloss by tf\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)#,\n",
    "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        xent_loss = K.mean(xent_loss)\n",
    "        kl_loss = K.mean(kl_loss)\n",
    "        return K.mean(xent_loss + kl_weight * kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "def kl_loss(x, x_decoded_mean):\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    kl_loss = kl_weight * kl_loss\n",
    "    return kl_loss\n",
    "    \n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01)\n",
    "\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### FOR TESTING PURPOSES BEFORE GRADUAL READ IN IS FIXED #####\n",
    "import random\n",
    "random_list = train_data[:2000]\n",
    "random_train, random_val = train_test_split(random_list, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n"
     ]
    }
   ],
   "source": [
    "print(len(random_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/envs/ai3sd/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      " 25/100 [======>.......................] - ETA: 23:55 - loss: 1506.5578"
     ]
    }
   ],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + model_name + \".h5\" \n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "    return checkpointer\n",
    "\n",
    "checkpointer = create_model_checkpoint('models', 'vae_seq2seq_new')\n",
    "\n",
    "#random_train = tf.cast(random_train, tf.int32)\n",
    "\n",
    "vae.fit(random_train, random_train,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=50, \n",
    "        validation_data=(random_val, random_val),\n",
    "        validation_steps=100,\n",
    "        callbacks=[checkpointer])\n",
    "    \n",
    "\n",
    "print(K.eval(vae.optimizer.lr))\n",
    "K.set_value(vae.optimizer.lr, 0.01)\n",
    "\n",
    "vae.save('models/vae_lstm_attempt2.h5')\n",
    "vae.load_weights('models/vae_seq2seq_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project sentences on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a generator that can sample sentences from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2char = {v: k for k, v in char_index.items()}\n",
    "index2char[0] = 'pad'\n",
    "\n",
    "mofid_index = 100\n",
    "mofid_encoded = encoder.predict(random_val)\n",
    "x_test_reconstructed = generator.predict(mofid_encoded, batch_size = 1)\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])\n",
    "\n",
    "print(reconstructed_indexes)\n",
    "\n",
    "word_list = list(np.vectorize(index2char.get)(reconstructed_indexes))\n",
    "word_list = ''.join(map(str, word_list))\n",
    "print('Reconstructed MOFid: ', word_list)\n",
    "\n",
    "#original_mofid = []\n",
    "#for i in range(len(validation_data[mofid_index])):\n",
    "#    original_mofid.append(index2char[np.argmax(random_val[mofid_index][i])])\n",
    "original_sent = list(np.vectorize(index2word.get)(random_val[mofid_index]))\n",
    "original_mofid = ''.join(map(str, original_mofid))\n",
    "print('Original MOFid: ',original_mofid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test_reconstructed))\n",
    "print(len(x_test_reconstructed[0]))\n",
    "print(len(x_test_reconstructed[0][0]))\n",
    "print(index2char)\n",
    "print(x_test_reconstructed[0][0])\n",
    "print(np.argmax(x_test_reconstructed[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
